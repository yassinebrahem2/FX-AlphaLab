{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cbe9f6",
   "metadata": {},
   "source": [
    "# Reddit r/Forex Retail Sentiment Analysis\n",
    "\n",
    "## The Question We're Trying to Answer\n",
    "\n",
    "Does retail trader chatter on Reddit predict anything about FX price direction or volatility?\n",
    "\n",
    "Reddit's r/Forex community — with over 800,000 members — is one of the largest public forums where retail traders discuss the foreign exchange market in real time. Every day, traders post their analyses, share trade ideas, ask questions about specific currency pairs, and express directional views on the market. The community also has recurring structured threads (\"What pairs are you trading this week?\", daily discussion threads) that aggregate positioning sentiment in a semi-structured way.\n",
    "\n",
    "The hypothesis is straightforward: **if retail sentiment is systematically wrong (the classic \"fade the crowd\" theory) or systematically right (herding on momentum), then aggregated Reddit sentiment should have a measurable correlation with subsequent price movements.** Even if the signal is weak on its own, it could serve as a complementary input to the Sentiment Agent when combined with institutional positioning data (COT) and central bank communication (news sentiment).\n",
    "\n",
    "---\n",
    "\n",
    "## Data Source: Arctic Shift API\n",
    "\n",
    "Reddit's public JSON endpoint now blocks unauthenticated requests (403 Blocked). Instead, we use the **Arctic Shift API** — a free, open archive of Reddit data maintained for researchers and moderators. Arctic Shift ingests Reddit's real-time firehose and makes the complete history searchable via a REST API.\n",
    "\n",
    "| Endpoint | Description |\n",
    "|---|---|\n",
    "| `/api/posts/search?subreddit=Forex` | Search r/Forex submissions with date/keyword filters |\n",
    "| `/api/comments/search?subreddit=Forex` | Search r/Forex comments with date/keyword filters |\n",
    "| `/api/posts/search/aggregate` | Aggregated statistics (post frequency, top authors) |\n",
    "\n",
    "**Key advantages over Reddit's own API**:\n",
    "- No API key / OAuth required\n",
    "- Full historical access (not limited to recent ~1,000 posts)\n",
    "- Date range filtering with `after` / `before` parameters\n",
    "- Keyword search in titles and selftext\n",
    "- Generous rate limits for normal use\n",
    "\n",
    "**Limitation**: Results are capped at 100 per request. We paginate by `created_utc` (ascending sort, sliding the `after` cursor forward) to collect large time windows. Score and comment counts may lag by ~36 hours for very recent posts.\n",
    "\n",
    "---\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "1. **Collect** 12+ months of r/Forex posts via Arctic Shift, paginating by date to maximize coverage\n",
    "2. **Clean** text data — strip markdown, URLs, special characters; extract currency pair mentions\n",
    "3. **Score sentiment** using FinBERT (BERT fine-tuned on financial text) — the same model used project-wide\n",
    "4. **Explore** community patterns — volume, pair focus, sentiment distributions, engagement\n",
    "5. **Assess signal quality** — does aggregated sentiment correlate with FX price movements?\n",
    "6. **Export** a model-ready dataset to the Silver layer for Sentiment Agent integration\n",
    "\n",
    "*Reference: The sentiment scoring approach is aligned with the project's existing news preprocessor (`src/ingestion/preprocessors/news_preprocessor.py`), which also uses FinBERT.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0345cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# FinBERT for financial-domain sentiment analysis (matches project NewsPreprocessor)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Auto-detect GPU availability\n",
    "_device = 0 if torch.cuda.is_available() else -1\n",
    "_device_name = 'GPU' if _device == 0 else 'CPU'\n",
    "print(f'Loading FinBERT sentiment model on {_device_name}...')\n",
    "\n",
    "finbert_model = hf_pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='ProsusAI/finbert',\n",
    "    tokenizer='ProsusAI/finbert',\n",
    "    device=_device,\n",
    ")\n",
    "print(f'\\u2713 FinBERT model loaded on {_device_name}')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "print(\"\\u2713 Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee06da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration\n",
    "BASE_PATH = Path('.').resolve().parent  # FX-AlphaLab root\n",
    "RAW_DIR = BASE_PATH / 'data' / 'raw' / 'reddit'\n",
    "PROCESSED_DIR = BASE_PATH / 'data' / 'processed' / 'sentiment'\n",
    "OHLCV_DIR = BASE_PATH / 'data' / 'processed' / 'ohlcv'\n",
    "\n",
    "# Create directories\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Visualization settings\n",
    "FIGSIZE_WIDE = (16, 6)\n",
    "FIGSIZE_TALL = (16, 10)\n",
    "FIGSIZE_SQUARE = (12, 8)\n",
    "\n",
    "# Arctic Shift configuration\n",
    "ARCTIC_SHIFT_BASE = 'https://arctic-shift.photon-reddit.com'\n",
    "SUBREDDIT = 'Forex'\n",
    "REQUEST_DELAY = 0.5  # seconds between requests (Arctic Shift is generous)\n",
    "\n",
    "# Collection window — 12 months of data for meaningful analysis\n",
    "COLLECT_AFTER = '2025-01-01'\n",
    "COLLECT_BEFORE = '2026-02-21'\n",
    "\n",
    "# FX pairs we track — regex patterns for mention detection\n",
    "FX_PAIRS = {\n",
    "    'EURUSD': r'(?:EUR[/\\\\\\s-]?USD|eurusd|eur\\s*usd|euro\\s*dollar)',\n",
    "    'GBPUSD': r'(?:GBP[/\\\\\\s-]?USD|gbpusd|gbp\\s*usd|cable|pound\\s*dollar)',\n",
    "    'USDJPY': r'(?:USD[/\\\\\\s-]?JPY|usdjpy|usd\\s*jpy|dollar\\s*yen|gopher)',\n",
    "    'USDCHF': r'(?:USD[/\\\\\\s-]?CHF|usdchf|usd\\s*chf|swissy)',\n",
    "    'AUDUSD': r'(?:AUD[/\\\\\\s-]?USD|audusd|aud\\s*usd|aussie)',\n",
    "    'USDCAD': r'(?:USD[/\\\\\\s-]?CAD|usdcad|usd\\s*cad|loonie)',\n",
    "    'NZDUSD': r'(?:NZD[/\\\\\\s-]?USD|nzdusd|nzd\\s*usd|kiwi)',\n",
    "    'EURJPY': r'(?:EUR[/\\\\\\s-]?JPY|eurjpy|eur\\s*jpy)',\n",
    "    'GBPJPY': r'(?:GBP[/\\\\\\s-]?JPY|gbpjpy|gbp\\s*jpy|guppy|beast)',\n",
    "    'EURGBP': r'(?:EUR[/\\\\\\s-]?GBP|eurgbp|eur\\s*gbp)',\n",
    "    'XAUUSD': r'(?:XAU[/\\\\\\s-]?USD|xauusd|xau\\s*usd|gold)',\n",
    "}\n",
    "\n",
    "# FinBERT batch inference settings\n",
    "FINBERT_BATCH_SIZE = 32\n",
    "FINBERT_MAX_LENGTH = 512\n",
    "\n",
    "print(\"\\u2713 Configuration complete\")\n",
    "print(f\"Arctic Shift base URL: {ARCTIC_SHIFT_BASE}\")\n",
    "print(f\"Collection window: {COLLECT_AFTER} to {COLLECT_BEFORE}\")\n",
    "print(f\"Raw data directory: {RAW_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DIR}\")\n",
    "print(f\"Tracking {len(FX_PAIRS)} FX pairs\")\n",
    "print(f\"FinBERT batch size: {FINBERT_BATCH_SIZE}, max tokens: {FINBERT_MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa9940",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We collect posts from r/Forex using the Arctic Shift API. The strategy is to paginate by ascending `created_utc` — each request returns up to 100 posts, and we slide the `after` cursor forward to the timestamp of the last post received. This allows us to walk through the full history of the subreddit within our target date range.\n",
    "\n",
    "Unlike Reddit's own API (which is now 403-blocked for unauthenticated requests), Arctic Shift provides:\n",
    "- **Full historical access** — posts going back to the creation of the subreddit\n",
    "- **Reliable pagination** — no cursor expiration or random ordering issues\n",
    "- **Keyword search** — optional `title` and `query` parameters for targeted collection\n",
    "\n",
    "We collect in two phases:\n",
    "1. **Broad sweep**: All r/Forex posts in the target date range (captures general sentiment)\n",
    "2. **Targeted search**: Posts mentioning specific FX pairs (enriches pair-level coverage)\n",
    "\n",
    "**Rate limiting**: Arctic Shift is generous for normal use (a few requests per second). We use a 0.5-second delay between requests to be respectful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arctic_shift_posts(\n",
    "    subreddit: str,\n",
    "    after: str,\n",
    "    before: str,\n",
    "    query: str | None = None,\n",
    "    title: str | None = None,\n",
    "    limit_per_request: int = 100,\n",
    "    max_posts: int = 10_000,\n",
    "    delay: float = REQUEST_DELAY,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Fetch posts from Arctic Shift API with date-cursor pagination.\n",
    "\n",
    "    Paginates by sliding the `after` parameter forward to the `created_utc`\n",
    "    of the last post in each batch. Stops when no more results are returned\n",
    "    or max_posts is reached.\n",
    "\n",
    "    Args:\n",
    "        subreddit: Subreddit name (without r/).\n",
    "        after: Start date (ISO 8601 or epoch).\n",
    "        before: End date (ISO 8601 or epoch).\n",
    "        query: Optional keyword search in title + selftext.\n",
    "        title: Optional keyword search in title only.\n",
    "        limit_per_request: Posts per API call (max 100).\n",
    "        max_posts: Safety cap on total posts collected.\n",
    "        delay: Seconds between requests.\n",
    "\n",
    "    Returns:\n",
    "        List of post dictionaries from Arctic Shift.\n",
    "    \"\"\"\n",
    "    all_posts = []\n",
    "    current_after = after\n",
    "    page = 0\n",
    "\n",
    "    while len(all_posts) < max_posts:\n",
    "        params = {\n",
    "            'subreddit': subreddit,\n",
    "            'after': current_after,\n",
    "            'before': before,\n",
    "            'limit': limit_per_request,\n",
    "            'sort': 'asc',\n",
    "        }\n",
    "        if query:\n",
    "            params['query'] = query\n",
    "        if title:\n",
    "            params['title'] = title\n",
    "\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f'{ARCTIC_SHIFT_BASE}/api/posts/search',\n",
    "                params=params,\n",
    "                timeout=30,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(f\"    Rate limited on page {page + 1}. Waiting 30s...\")\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "            print(f\"    HTTP error on page {page + 1}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"    Error on page {page + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Arctic Shift returns {\"data\": [...]}\n",
    "        posts = data.get('data', [])\n",
    "        if not posts:\n",
    "            break\n",
    "\n",
    "        all_posts.extend(posts)\n",
    "        page += 1\n",
    "\n",
    "        # Slide cursor forward to the last post's timestamp\n",
    "        last_created = posts[-1].get('created_utc', 0)\n",
    "        if isinstance(last_created, (int, float)):\n",
    "            # Add 1 second to avoid re-fetching the same post\n",
    "            current_after = str(int(last_created) + 1)\n",
    "        else:\n",
    "            current_after = last_created\n",
    "\n",
    "        # Progress reporting\n",
    "        if page % 10 == 0:\n",
    "            print(f\"    ... page {page}, {len(all_posts)} posts so far\")\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return all_posts\n",
    "\n",
    "\n",
    "print(\"\\u2713 Collection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67805ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute data collection\n",
    "print(\"=\" * 80)\n",
    "print(\"COLLECTING r/Forex DATA VIA ARCTIC SHIFT API\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for existing raw data first (cache to avoid re-collecting)\n",
    "existing_raw = sorted(RAW_DIR.glob('reddit_forex_arctic_*.json'), reverse=True)\n",
    "if existing_raw:\n",
    "    latest_raw = existing_raw[0]\n",
    "    print(f\"\\n\\u2713 Found existing raw data: {latest_raw.name}\")\n",
    "    print(\"  Loading from cache to avoid re-collecting...\")\n",
    "    with open(latest_raw, encoding='utf-8') as f:\n",
    "        all_posts_raw = json.load(f)\n",
    "    print(f\"  Loaded {len(all_posts_raw)} cached posts\")\n",
    "else:\n",
    "    all_posts_raw = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    # Phase 1: Broad sweep — all r/Forex posts in date range\n",
    "    print(f\"\\nPhase 1: Broad sweep ({COLLECT_AFTER} to {COLLECT_BEFORE})...\")\n",
    "    broad_posts = fetch_arctic_shift_posts(\n",
    "        subreddit=SUBREDDIT,\n",
    "        after=COLLECT_AFTER,\n",
    "        before=COLLECT_BEFORE,\n",
    "        max_posts=15_000,\n",
    "    )\n",
    "    for post in broad_posts:\n",
    "        pid = post.get('id', '')\n",
    "        if pid and pid not in seen_ids:\n",
    "            seen_ids.add(pid)\n",
    "            all_posts_raw.append(post)\n",
    "    print(f\"  \\u2713 Broad sweep: {len(broad_posts)} fetched, {len(all_posts_raw)} unique\")\n",
    "\n",
    "    # Phase 2: Targeted search — specific pair and positioning queries\n",
    "    search_queries = [\n",
    "        'EURUSD',\n",
    "        'GBPUSD',\n",
    "        'USDJPY',\n",
    "        'XAUUSD gold',\n",
    "        'bullish bearish',\n",
    "        'long short position',\n",
    "        'weekly forecast',\n",
    "    ]\n",
    "\n",
    "    print(\"\\nPhase 2: Targeted search queries...\")\n",
    "    for q in search_queries:\n",
    "        print(f\"  Searching: '{q}'...\")\n",
    "        results = fetch_arctic_shift_posts(\n",
    "            subreddit=SUBREDDIT,\n",
    "            after=COLLECT_AFTER,\n",
    "            before=COLLECT_BEFORE,\n",
    "            query=q,\n",
    "            max_posts=2_000,\n",
    "        )\n",
    "        new_count = 0\n",
    "        for post in results:\n",
    "            pid = post.get('id', '')\n",
    "            if pid and pid not in seen_ids:\n",
    "                seen_ids.add(pid)\n",
    "                all_posts_raw.append(post)\n",
    "                new_count += 1\n",
    "        print(f\"    \\u2713 {len(results)} results, {new_count} new\")\n",
    "\n",
    "    # Save raw data to Bronze layer\n",
    "    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    raw_path = RAW_DIR / f'reddit_forex_arctic_{timestamp_str}.json'\n",
    "    with open(raw_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_posts_raw, f, ensure_ascii=False, default=str)\n",
    "    print(f\"\\n\\u2713 Raw data saved to Bronze layer: {raw_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\u2713 Total unique posts collected: {len(all_posts_raw)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b374a2c",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning & Quality Assessment\n",
    "\n",
    "Reddit posts are noisy. They contain markdown formatting, embedded URLs, emoji, automated bot messages, and highly variable writing quality. Before we can extract meaningful sentiment, we need to:\n",
    "\n",
    "1. **Parse** the raw JSON into a structured DataFrame with consistent fields\n",
    "2. **Clean** text — strip markdown, URLs, special characters while preserving financial terminology\n",
    "3. **Extract** currency pair mentions from titles and body text using regex patterns\n",
    "4. **Filter** out low-quality content — bot posts, deleted accounts, extremely short posts\n",
    "5. **Validate** data quality — missing values, duplicates, date distribution\n",
    "\n",
    "The text cleaning approach is informed by the project's `NewsPreprocessor` pattern: normalize text, generate deterministic article IDs, and map to the Silver sentiment schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean Reddit post/comment text for sentiment analysis.\n",
    "\n",
    "    Removes markdown formatting, URLs, excessive whitespace while preserving\n",
    "    financial terminology and pair symbols.\n",
    "\n",
    "    Args:\n",
    "        text: Raw Reddit text (markdown formatted).\n",
    "\n",
    "    Returns:\n",
    "        Cleaned plain text string.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Skip Reddit's deleted/removed placeholders\n",
    "    if text in ('[deleted]', '[removed]'):\n",
    "        return ''\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # Remove markdown links [text](url)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    # Remove markdown formatting (bold, italic, headers, quotes)\n",
    "    text = re.sub(r'[*#>~`]', '', text)\n",
    "    # Remove Reddit-specific HTML entities\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    text = re.sub(r'&#x200B;', '', text)\n",
    "    # Remove image/media references\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_pairs(text: str) -> list[str]:\n",
    "    \"\"\"Extract FX pair mentions from text using regex patterns.\n",
    "\n",
    "    Args:\n",
    "        text: Cleaned text to search.\n",
    "\n",
    "    Returns:\n",
    "        List of detected pair symbols (e.g., ['EURUSD', 'GBPJPY']).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    found = []\n",
    "    for pair, pattern in FX_PAIRS.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            found.append(pair)\n",
    "    return found\n",
    "\n",
    "\n",
    "def generate_article_id(url: str, title: str, timestamp: str, source: str) -> str:\n",
    "    \"\"\"Generate a deterministic 16-char article ID (matching NewsPreprocessor pattern).\n",
    "\n",
    "    Args:\n",
    "        url: Post permalink or URL.\n",
    "        title: Post title.\n",
    "        timestamp: ISO 8601 timestamp string.\n",
    "        source: Source identifier.\n",
    "\n",
    "    Returns:\n",
    "        16-character hex hash string.\n",
    "    \"\"\"\n",
    "    key = url if url else f\"{title}_{timestamp}_{source}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "print(\"\\u2713 Cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw posts into structured DataFrame\n",
    "print(\"Parsing raw posts into structured DataFrame...\")\n",
    "\n",
    "records = []\n",
    "bot_accounts = {'AutoModerator', '[deleted]', 'FXGears', 'forex_bot', 'AutoNewspaperAdmin'}\n",
    "\n",
    "for post in all_posts_raw:\n",
    "    author = post.get('author', '[deleted]') or '[deleted]'\n",
    "\n",
    "    # Skip bot posts and deleted accounts\n",
    "    if author in bot_accounts:\n",
    "        continue\n",
    "\n",
    "    # Extract fields — Arctic Shift uses same field names as Reddit's API\n",
    "    title_raw = post.get('title', '')\n",
    "    body_raw = post.get('selftext', '') or ''\n",
    "    created_utc = post.get('created_utc', 0)\n",
    "    score = post.get('score', 0) or 0\n",
    "    num_comments = post.get('num_comments', 0) or 0\n",
    "    permalink = post.get('permalink', '')\n",
    "    flair = post.get('link_flair_text', '') or ''\n",
    "    post_id = post.get('id', '')\n",
    "\n",
    "    # Clean text\n",
    "    title_clean = clean_text(title_raw)\n",
    "    body_clean = clean_text(body_raw)\n",
    "    combined_text = f\"{title_clean} {body_clean}\".strip()\n",
    "\n",
    "    # Skip empty or ultra-short posts (likely images/links without text)\n",
    "    if len(combined_text) < 10:\n",
    "        continue\n",
    "\n",
    "    # Extract pair mentions\n",
    "    pairs = extract_pairs(combined_text)\n",
    "\n",
    "    # Generate timestamp — Arctic Shift stores created_utc as epoch seconds\n",
    "    if isinstance(created_utc, (int, float)) and created_utc > 0:\n",
    "        timestamp = datetime.fromtimestamp(created_utc, tz=timezone.utc)\n",
    "    else:\n",
    "        # Try parsing as ISO string\n",
    "        try:\n",
    "            timestamp = pd.to_datetime(created_utc, utc=True).to_pydatetime()\n",
    "        except Exception:\n",
    "            continue  # Skip posts without valid timestamps\n",
    "\n",
    "    timestamp_str = timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Generate article ID\n",
    "    url = f\"https://www.reddit.com{permalink}\" if permalink else ''\n",
    "    article_id = generate_article_id(url, title_clean, timestamp_str, 'reddit')\n",
    "\n",
    "    records.append({\n",
    "        'timestamp_utc': timestamp_str,\n",
    "        'article_id': article_id,\n",
    "        'post_id': post_id,\n",
    "        'headline': title_clean,\n",
    "        'body': body_clean,\n",
    "        'combined_text': combined_text,\n",
    "        'pairs_mentioned': pairs,\n",
    "        'primary_pair': pairs[0] if pairs else None,\n",
    "        'flair': flair if flair else None,\n",
    "        'score': score,\n",
    "        'num_comments': num_comments,\n",
    "        'author': author,\n",
    "        'url': url,\n",
    "        'text_length': len(combined_text),\n",
    "    })\n",
    "\n",
    "df_posts = pd.DataFrame(records)\n",
    "df_posts['timestamp_utc_dt'] = pd.to_datetime(df_posts['timestamp_utc'])\n",
    "df_posts = df_posts.sort_values('timestamp_utc_dt').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\u2713 Parsed {len(df_posts)} posts (excluded bots and empty posts)\")\n",
    "if len(df_posts) > 0:\n",
    "    print(f\"Date range: {df_posts['timestamp_utc_dt'].min()} to {df_posts['timestamp_utc_dt'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality assessment\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Missing values\n",
    "print(\"\\n1. MISSING VALUES\")\n",
    "key_cols = ['timestamp_utc', 'article_id', 'headline', 'combined_text', 'score']\n",
    "missing = df_posts[key_cols].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\\u2713 No missing values in key fields\")\n",
    "\n",
    "# 2. Duplicates\n",
    "print(\"\\n2. DUPLICATE CHECK\")\n",
    "dupes = df_posts.duplicated(subset=['article_id']).sum()\n",
    "if dupes > 0:\n",
    "    print(f\"\\u26A0 Found {dupes} duplicate article_ids \\u2014 removing...\")\n",
    "    df_posts = df_posts.drop_duplicates(subset=['article_id'], keep='first').reset_index(drop=True)\n",
    "    print(f\"\\u2713 After dedup: {len(df_posts)} posts\")\n",
    "else:\n",
    "    print(f\"\\u2713 No duplicates detected ({len(df_posts)} unique posts)\")\n",
    "\n",
    "# 3. Date distribution\n",
    "print(\"\\n3. DATE DISTRIBUTION\")\n",
    "df_posts['date'] = df_posts['timestamp_utc_dt'].dt.date\n",
    "date_range = (df_posts['timestamp_utc_dt'].max() - df_posts['timestamp_utc_dt'].min()).days\n",
    "print(f\"Date span: {date_range} days\")\n",
    "print(f\"Earliest post: {df_posts['timestamp_utc_dt'].min()}\")\n",
    "print(f\"Latest post:   {df_posts['timestamp_utc_dt'].max()}\")\n",
    "print(\"\\nPosts per month:\")\n",
    "monthly = df_posts.groupby(df_posts['timestamp_utc_dt'].dt.to_period('M')).size()\n",
    "print(monthly)\n",
    "\n",
    "# 4. Content quality\n",
    "print(\"\\n4. CONTENT QUALITY\")\n",
    "print(f\"Median text length: {df_posts['text_length'].median():.0f} chars\")\n",
    "print(f\"Mean text length:   {df_posts['text_length'].mean():.0f} chars\")\n",
    "print(f\"Posts with body text: {(df_posts['body'].str.len() > 0).sum()} ({(df_posts['body'].str.len() > 0).mean()*100:.1f}%)\")\n",
    "print(f\"Posts mentioning FX pairs: {df_posts['primary_pair'].notna().sum()} ({df_posts['primary_pair'].notna().mean()*100:.1f}%)\")\n",
    "\n",
    "# 5. Engagement statistics\n",
    "print(\"\\n5. ENGAGEMENT STATISTICS\")\n",
    "print(f\"Score \\u2014 median: {df_posts['score'].median():.0f}, mean: {df_posts['score'].mean():.1f}, max: {df_posts['score'].max()}\")\n",
    "print(f\"Comments \\u2014 median: {df_posts['num_comments'].median():.0f}, mean: {df_posts['num_comments'].mean():.1f}, max: {df_posts['num_comments'].max()}\")\n",
    "\n",
    "# 6. Flair distribution\n",
    "print(\"\\n6. POST FLAIR DISTRIBUTION\")\n",
    "flair_counts = df_posts['flair'].value_counts().head(10)\n",
    "if len(flair_counts) > 0:\n",
    "    print(flair_counts)\n",
    "else:\n",
    "    print(\"No flair data available\")\n",
    "\n",
    "print(\"\\n\\u2713 Data quality assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff3646",
   "metadata": {},
   "source": [
    "### Quality Assessment Observations\n",
    "\n",
    "Reddit data is fundamentally different from our other sources (CFTC, FRED, central bank publications). Key quality characteristics:\n",
    "\n",
    "- **No structural gaps** in key fields, but content depth varies enormously — some posts are 2,000-word technical analyses, others are 10-word questions\n",
    "- **FX pair coverage is partial** — not every post mentions a specific pair. Many posts discuss general trading psychology, risk management, or broker questions. Posts without pair mentions still carry market sentiment but are harder to map to specific instruments\n",
    "- **Engagement follows a power law** — a small number of posts attract most of the upvotes and comments. High-engagement posts are more likely to represent community consensus\n",
    "- **Flair categories** help distinguish analysis posts from beginner questions and memes — useful for filtering during signal construction\n",
    "\n",
    "These characteristics mean the dataset is inherently noisier than institutional data sources. The EDA section will quantify how much usable signal exists within this noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcfa44",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "We score each post using **FinBERT** (`ProsusAI/finbert`) — a BERT language model fine-tuned on financial text. This is the same model used by the project's `NewsPreprocessor` for central bank communications, ensuring consistency across all sentiment sources in the pipeline.\n",
    "\n",
    "FinBERT outputs three labels (positive, negative, neutral) with a confidence score. We convert this to a continuous score in [-1.0, 1.0]:\n",
    "- **positive** → `+confidence`\n",
    "- **negative** → `-confidence`\n",
    "- **neutral** → `0.0`\n",
    "\n",
    "The model processes texts in batches of 32, truncated to 512 tokens. For Reddit posts, we score the concatenated title + body text. FinBERT's financial vocabulary naturally handles terms like \"bullish\", \"bearish\", \"hawkish\", and \"dovish\" that generic sentiment tools miss.\n",
    "\n",
    "**Why FinBERT for Reddit text?** While FinBERT was trained on formal financial prose, FX-specific terminology (pair names, directional language, monetary policy terms) appears frequently in r/Forex posts. Using the same model across all sources ensures calibrated, comparable scores. The model's 512-token window also handles Reddit's longer analytical posts well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c95944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment_finbert_batch(\n",
    "    texts: list[str],\n",
    "    model: object,\n",
    "    batch_size: int = FINBERT_BATCH_SIZE,\n",
    "    max_length: int = FINBERT_MAX_LENGTH,\n",
    ") -> tuple[list[float], list[str]]:\n",
    "    \"\"\"Score sentiment for a batch of texts using FinBERT.\n",
    "\n",
    "    Matches the scoring logic in NewsPreprocessor._analyze_sentiment_batch:\n",
    "    - positive -> +confidence\n",
    "    - negative -> -confidence\n",
    "    - neutral  -> 0.0\n",
    "\n",
    "    Args:\n",
    "        texts: List of cleaned text strings.\n",
    "        model: Hugging Face sentiment-analysis pipeline (FinBERT).\n",
    "        batch_size: Texts per model call.\n",
    "        max_length: Max tokens per text (truncated).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (scores, labels) parallel to input texts.\n",
    "        - scores: Float in [-1.0, 1.0]\n",
    "        - labels: 'positive', 'neutral', or 'negative'\n",
    "    \"\"\"\n",
    "    scores = [0.0] * len(texts)\n",
    "    labels = ['neutral'] * len(texts)\n",
    "\n",
    "    # Separate empty texts (skip model) from non-empty (batch score)\n",
    "    non_empty_indices = [i for i, t in enumerate(texts) if t and len(t.strip()) > 0]\n",
    "    non_empty_texts = [texts[i] for i in non_empty_indices]\n",
    "\n",
    "    if not non_empty_texts:\n",
    "        return scores, labels\n",
    "\n",
    "    try:\n",
    "        results = model(\n",
    "            non_empty_texts,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        for idx, result in zip(non_empty_indices, results):\n",
    "            label = result['label'].lower()\n",
    "            confidence = result['score']\n",
    "\n",
    "            if label == 'positive':\n",
    "                score = confidence\n",
    "            elif label == 'negative':\n",
    "                score = -confidence\n",
    "            else:  # neutral\n",
    "                score = 0.0\n",
    "\n",
    "            scores[idx] = round(score, 4)\n",
    "            labels[idx] = label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\u26A0 Batch sentiment analysis failed: {e}\")\n",
    "        print(\"  Falling back to per-text scoring...\")\n",
    "        for idx in non_empty_indices:\n",
    "            try:\n",
    "                result = model(\n",
    "                    texts[idx],\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                )[0]\n",
    "                label = result['label'].lower()\n",
    "                confidence = result['score']\n",
    "\n",
    "                if label == 'positive':\n",
    "                    score = confidence\n",
    "                elif label == 'negative':\n",
    "                    score = -confidence\n",
    "                else:\n",
    "                    score = 0.0\n",
    "\n",
    "                scores[idx] = round(score, 4)\n",
    "                labels[idx] = label\n",
    "            except Exception:\n",
    "                pass  # Leave as neutral/0.0\n",
    "\n",
    "    return scores, labels\n",
    "\n",
    "\n",
    "# Apply FinBERT sentiment scoring\n",
    "print(\"Scoring sentiment for all posts using FinBERT...\")\n",
    "print(f\"  Batch size: {FINBERT_BATCH_SIZE}, max tokens: {FINBERT_MAX_LENGTH}\")\n",
    "print(f\"  Total texts: {len(df_posts)}\")\n",
    "\n",
    "all_texts = df_posts['combined_text'].tolist()\n",
    "finbert_scores, finbert_labels = score_sentiment_finbert_batch(all_texts, finbert_model)\n",
    "\n",
    "df_posts['sentiment_score'] = finbert_scores\n",
    "df_posts['sentiment_label'] = finbert_labels\n",
    "\n",
    "# Summary\n",
    "print(\"\\u2713 Sentiment scored using FinBERT (ProsusAI/finbert)\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_dist = df_posts['sentiment_label'].value_counts()\n",
    "for label, count in label_dist.items():\n",
    "    pct = count / len(df_posts) * 100\n",
    "    print(f\"  {label:>8}: {count:>5} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nScore statistics:\")\n",
    "print(f\"  Mean:   {df_posts['sentiment_score'].mean():+.4f}\")\n",
    "print(f\"  Median: {df_posts['sentiment_score'].median():+.4f}\")\n",
    "print(f\"  Std:    {df_posts['sentiment_score'].std():.4f}\")\n",
    "print(f\"  Min:    {df_posts['sentiment_score'].min():+.4f}\")\n",
    "print(f\"  Max:    {df_posts['sentiment_score'].max():+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add37e47",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "With sentiment scores assigned, we can now explore *what* the r/Forex community talks about, *how* they feel about it, and *whether* any of these patterns are structured enough to serve as inputs to the Sentiment Agent.\n",
    "\n",
    "We examine four dimensions:\n",
    "1. **Posting volume & activity patterns** — when does the community post, and does volume correlate with market events?\n",
    "2. **Pair focus** — which pairs dominate discussion, and does each pair have a sentiment bias?\n",
    "3. **Sentiment distributions** — is sentiment normally distributed or skewed? Are there regime differences?\n",
    "4. **Engagement vs sentiment** — do bullish or bearish posts get more engagement? This reveals community bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c78c",
   "metadata": {},
   "source": [
    "### 4.1 Posting Volume & Activity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c299843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting volume by week\n",
    "fig, axes = plt.subplots(2, 1, figsize=FIGSIZE_WIDE)\n",
    "fig.suptitle('r/Forex Posting Activity', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Weekly post count\n",
    "ax = axes[0]\n",
    "weekly_counts = df_posts.set_index('timestamp_utc_dt').resample('W').size()\n",
    "ax.bar(weekly_counts.index, weekly_counts.values, color='steelblue', alpha=0.7, width=5)\n",
    "ax.set_ylabel('Posts per Week', fontsize=11)\n",
    "ax.set_title('Weekly Post Volume', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly mean sentiment\n",
    "ax = axes[1]\n",
    "weekly_sentiment = df_posts.set_index('timestamp_utc_dt')['sentiment_score'].resample('W').mean()\n",
    "colors = ['green' if s >= 0 else 'red' for s in weekly_sentiment.values]\n",
    "ax.bar(weekly_sentiment.index, weekly_sentiment.values, color=colors, alpha=0.7, width=5)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_ylabel('Mean Sentiment Score', fontsize=11)\n",
    "ax.set_title('Weekly Average Sentiment', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\u2713 Posting volume visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Day-of-week and hour patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_WIDE)\n",
    "fig.suptitle('r/Forex Activity Patterns', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Day of week\n",
    "ax = axes[0]\n",
    "df_posts['day_of_week'] = df_posts['timestamp_utc_dt'].dt.day_name()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = df_posts['day_of_week'].value_counts().reindex(day_order)\n",
    "\n",
    "bar_colors = ['#2196F3' if d in ['Monday','Tuesday','Wednesday','Thursday','Friday'] else '#9E9E9E' for d in day_order]\n",
    "ax.bar(range(7), day_counts.values, color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels([d[:3] for d in day_order], fontsize=10)\n",
    "ax.set_ylabel('Post Count', fontsize=11)\n",
    "ax.set_title('Posts by Day of Week', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Hour of day (UTC)\n",
    "ax = axes[1]\n",
    "df_posts['hour_utc'] = df_posts['timestamp_utc_dt'].dt.hour\n",
    "hour_counts = df_posts['hour_utc'].value_counts().sort_index()\n",
    "hour_counts = hour_counts.reindex(range(24), fill_value=0)\n",
    "\n",
    "# Color by trading session\n",
    "session_colors = []\n",
    "for h in range(24):\n",
    "    if 7 <= h <= 16:  # London session (UTC)\n",
    "        session_colors.append('#E91E63')\n",
    "    elif 13 <= h <= 21:  # NY overlap\n",
    "        session_colors.append('#FF9800')\n",
    "    elif 0 <= h <= 8:  # Asian session (UTC)\n",
    "        session_colors.append('#4CAF50')\n",
    "    else:\n",
    "        session_colors.append('#9E9E9E')\n",
    "\n",
    "ax.bar(range(24), hour_counts.values, color=session_colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Hour (UTC)', fontsize=11)\n",
    "ax.set_ylabel('Post Count', fontsize=11)\n",
    "ax.set_title('Posts by Hour (UTC)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(0, 24, 2))\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add session legend\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#4CAF50', alpha=0.7, label='Asian Session'),\n",
    "    Patch(facecolor='#E91E63', alpha=0.7, label='London Session'),\n",
    "    Patch(facecolor='#FF9800', alpha=0.7, label='NY Overlap'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=9, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\u2713 Activity pattern visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c9eec",
   "metadata": {},
   "source": [
    "### Activity Pattern Observations\n",
    "\n",
    "The posting patterns reveal that r/Forex activity is driven by the market calendar:\n",
    "\n",
    "- **Weekday-heavy posting** — most activity occurs Monday through Friday when FX markets are open, with a noticeable drop on weekends. This is a positive sign for signal quality: the community is reactive to live market conditions, not posting randomly.\n",
    "- **Peak hours align with trading sessions** — posting activity clusters around European and US market hours (roughly 12:00–20:00 UTC), when the most liquid FX sessions overlap. Asian session hours show lower activity, consistent with the subreddit's predominantly Western user base.\n",
    "- **Sentiment fluctuates weekly** — the sentiment time series shows variation that *could* be market-reactive. Whether these fluctuations predict anything is the question we'll address in Section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457315b",
   "metadata": {},
   "source": [
    "### 4.2 Currency Pair Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which pairs does r/Forex discuss most?\n",
    "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_WIDE)\n",
    "fig.suptitle('r/Forex Currency Pair Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Count all pair mentions (a post can mention multiple pairs)\n",
    "all_mentions = []\n",
    "for pairs_list in df_posts['pairs_mentioned']:\n",
    "    all_mentions.extend(pairs_list)\n",
    "\n",
    "pair_counts = Counter(all_mentions)\n",
    "pair_df = pd.DataFrame(pair_counts.most_common(), columns=['pair', 'mentions'])\n",
    "\n",
    "# Bar chart of pair mentions\n",
    "ax = axes[0]\n",
    "if len(pair_df) > 0:\n",
    "    bars = ax.barh(pair_df['pair'][::-1], pair_df['mentions'][::-1], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Number of Mentions', fontsize=11)\n",
    "    ax.set_title('Most Discussed Pairs', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No pair mentions detected', transform=ax.transAxes, ha='center')\n",
    "\n",
    "# Sentiment by pair\n",
    "ax = axes[1]\n",
    "\n",
    "# Explode pairs_mentioned into one row per pair\n",
    "df_pairs_exploded = df_posts.explode('pairs_mentioned').dropna(subset=['pairs_mentioned'])\n",
    "if len(df_pairs_exploded) > 0:\n",
    "    pair_sentiment = df_pairs_exploded.groupby('pairs_mentioned')['sentiment_score'].agg(['mean', 'std', 'count'])\n",
    "    pair_sentiment = pair_sentiment.sort_values('count', ascending=False)\n",
    "\n",
    "    # Only show pairs with at least 5 mentions\n",
    "    pair_sentiment_sig = pair_sentiment[pair_sentiment['count'] >= 5]\n",
    "\n",
    "    if len(pair_sentiment_sig) > 0:\n",
    "        colors_bar = ['green' if m >= 0 else 'red' for m in pair_sentiment_sig['mean']]\n",
    "        ax.barh(\n",
    "            pair_sentiment_sig.index[::-1],\n",
    "            pair_sentiment_sig['mean'][::-1],\n",
    "            xerr=pair_sentiment_sig['std'][::-1] / np.sqrt(pair_sentiment_sig['count'][::-1]),\n",
    "            color=colors_bar[::-1], alpha=0.7, edgecolor='black', capsize=3\n",
    "        )\n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax.set_xlabel('Mean Sentiment Score (\\u00b1 SEM)', fontsize=11)\n",
    "        ax.set_title('Sentiment by Pair (\\u22655 mentions)', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Not enough pair mentions for analysis', transform=ax.transAxes, ha='center')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No pair mentions detected', transform=ax.transAxes, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "posts_with_pairs = df_posts['primary_pair'].notna().sum()\n",
    "print(f\"\\nPosts mentioning specific pairs: {posts_with_pairs}/{len(df_posts)} ({posts_with_pairs/len(df_posts)*100:.1f}%)\")\n",
    "if len(pair_df) > 0:\n",
    "    print(\"\\nTop 5 discussed pairs:\")\n",
    "    for _, row in pair_df.head(5).iterrows():\n",
    "        print(f\"  {row['pair']:>8}: {row['mentions']} mentions\")\n",
    "print(\"\\n\\u2713 Pair analysis visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e999bdd",
   "metadata": {},
   "source": [
    "### Pair Focus Observations\n",
    "\n",
    "The pair mention analysis reveals the community's attention structure:\n",
    "\n",
    "- **XAUUSD (Gold) typically dominates** — a reflection of the r/Forex community's evolving focus. Many retail traders now include gold alongside traditional FX pairs, driven by volatility and clear trending behavior. This is relevant because gold sentiment may act as a risk-appetite proxy.\n",
    "- **Major USD pairs lead** — EURUSD, GBPUSD, and USDJPY are the most discussed traditional FX pairs, as expected given their liquidity and popularity among retail traders.\n",
    "- **Cross pairs get minimal attention** — EURGBP, EURJPY, and other crosses receive significantly fewer mentions, reflecting the retail tendency to focus on the most visible and heavily marketed pairs.\n",
    "- **Sentiment varies by pair** — some pairs have a persistent sentiment bias in the community. These biases (bullish or bearish) may reflect genuine positioning or may be lagging indicators of recent price action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075671d",
   "metadata": {},
   "source": [
    "### 4.3 Sentiment Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=FIGSIZE_SQUARE)\n",
    "fig.suptitle('Sentiment Score Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Overall distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(df_posts['sentiment_score'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n",
    "ax.axvline(x=df_posts['sentiment_score'].mean(), color='orange', linestyle='--', linewidth=1.5,\n",
    "           label=f\"Mean: {df_posts['sentiment_score'].mean():+.3f}\")\n",
    "ax.set_xlabel('Sentiment Score', fontsize=10)\n",
    "ax.set_ylabel('Frequency', fontsize=10)\n",
    "ax.set_title('Overall Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Label breakdown (pie)\n",
    "ax = axes[0, 1]\n",
    "label_counts = df_posts['sentiment_label'].value_counts()\n",
    "colors_pie = {'positive': '#4CAF50', 'neutral': '#9E9E9E', 'negative': '#F44336'}\n",
    "ax.pie(\n",
    "    label_counts.values,\n",
    "    labels=label_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=[colors_pie.get(lbl, '#9E9E9E') for lbl in label_counts.index],\n",
    "    startangle=90,\n",
    ")\n",
    "ax.set_title('Sentiment Label Split', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Sentiment by text length\n",
    "ax = axes[1, 0]\n",
    "df_posts['text_length_bin'] = pd.cut(df_posts['text_length'], bins=[0, 50, 200, 500, 2000, 100000],\n",
    "                                      labels=['<50', '50-200', '200-500', '500-2K', '2K+'])\n",
    "length_sentiment = df_posts.groupby('text_length_bin', observed=True)['sentiment_score'].mean()\n",
    "colors_len = ['green' if s >= 0 else 'red' for s in length_sentiment.values]\n",
    "length_sentiment.plot(kind='bar', ax=ax, color=colors_len, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_xlabel('Text Length (chars)', fontsize=10)\n",
    "ax.set_ylabel('Mean Sentiment', fontsize=10)\n",
    "ax.set_title('Sentiment by Post Length', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sentiment by flair\n",
    "ax = axes[1, 1]\n",
    "flair_sentiment = df_posts.groupby('flair')['sentiment_score'].agg(['mean', 'count'])\n",
    "flair_sentiment = flair_sentiment[flair_sentiment['count'] >= 5].sort_values('mean', ascending=True)\n",
    "\n",
    "if len(flair_sentiment) > 0:\n",
    "    colors_flair = ['green' if m >= 0 else 'red' for m in flair_sentiment['mean']]\n",
    "    ax.barh(flair_sentiment.index[-10:], flair_sentiment['mean'][-10:],\n",
    "            color=colors_flair[-10:], alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.set_xlabel('Mean Sentiment', fontsize=10)\n",
    "    ax.set_title('Sentiment by Flair (top 10)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Not enough flair data', transform=ax.transAxes, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\u2713 Sentiment distribution visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc50c73",
   "metadata": {},
   "source": [
    "### Sentiment Distribution Observations\n",
    "\n",
    "The distributions reveal the community's emotional structure:\n",
    "\n",
    "- **Positive skew overall** — Reddit's r/Forex community tends toward positive/neutral sentiment. This partly reflects the community's optimism bias (traders posting about expected wins rather than documenting losses) and partly the fact that analytical posts with balanced arguments score near-neutral under FinBERT.\n",
    "- **Longer posts tend toward neutral** — as post length increases, sentiment gravitates toward zero. This makes sense: detailed analytical posts balance bullish and bearish arguments, while short posts are more likely to be strongly directional declarations.\n",
    "- **Flair matters** — different post categories carry meaningfully different sentiment. \"Technical Analysis\" posts tend toward neutral (balanced assessment), while \"Trade Idea\" posts are more directional. This can be used as a filtering dimension during signal construction.\n",
    "\n",
    "The positive skew is important to note: any signal derived from this data should be calibrated against this baseline. A weekly average slightly above zero is not \"bullish\" — it's the community's default mood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb3689",
   "metadata": {},
   "source": [
    "### 4.4 Engagement vs Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Engagement vs Sentiment', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Score vs sentiment\n",
    "ax = axes[0]\n",
    "ax.scatter(df_posts['sentiment_score'], df_posts['score'],\n",
    "           alpha=0.3, s=10, color='steelblue')\n",
    "ax.set_xlabel('Sentiment Score', fontsize=11)\n",
    "ax.set_ylabel('Post Score (Upvotes)', fontsize=11)\n",
    "ax.set_title('Upvotes vs Sentiment', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('symlog', linthresh=10)\n",
    "\n",
    "# Comments vs sentiment\n",
    "ax = axes[1]\n",
    "ax.scatter(df_posts['sentiment_score'], df_posts['num_comments'],\n",
    "           alpha=0.3, s=10, color='coral')\n",
    "ax.set_xlabel('Sentiment Score', fontsize=11)\n",
    "ax.set_ylabel('Number of Comments', fontsize=11)\n",
    "ax.set_title('Comments vs Sentiment', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('symlog', linthresh=10)\n",
    "\n",
    "# Mean engagement by sentiment bucket\n",
    "ax = axes[2]\n",
    "df_posts['sentiment_bucket'] = pd.cut(\n",
    "    df_posts['sentiment_score'],\n",
    "    bins=[-1.0, -0.5, -0.05, 0.05, 0.5, 1.0],\n",
    "    labels=['Strong Neg', 'Weak Neg', 'Neutral', 'Weak Pos', 'Strong Pos']\n",
    ")\n",
    "bucket_engagement = df_posts.groupby('sentiment_bucket', observed=True).agg({\n",
    "    'score': 'mean',\n",
    "    'num_comments': 'mean',\n",
    "}).round(1)\n",
    "\n",
    "x = range(len(bucket_engagement))\n",
    "width = 0.35\n",
    "ax.bar([i - width/2 for i in x], bucket_engagement['score'], width, label='Avg Upvotes', color='steelblue', alpha=0.7)\n",
    "ax.bar([i + width/2 for i in x], bucket_engagement['num_comments'], width, label='Avg Comments', color='coral', alpha=0.7)\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(bucket_engagement.index, rotation=30, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Mean Value', fontsize=11)\n",
    "ax.set_title('Engagement by Sentiment Bucket', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between sentiment and engagement\n",
    "corr_score = df_posts['sentiment_score'].corr(df_posts['score'])\n",
    "corr_comments = df_posts['sentiment_score'].corr(df_posts['num_comments'])\n",
    "print(f\"Correlation: Sentiment vs Upvotes:  \\u03c1 = {corr_score:+.4f}\")\n",
    "print(f\"Correlation: Sentiment vs Comments: \\u03c1 = {corr_comments:+.4f}\")\n",
    "print(\"\\n\\u2713 Engagement analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c71e9",
   "metadata": {},
   "source": [
    "### Engagement Observations\n",
    "\n",
    "The relationship between sentiment and engagement reveals how the community reacts to different opinions:\n",
    "\n",
    "- **Neutral posts get the most engagement** — detailed analytical content that balances arguments attracts more upvotes and discussion than strongly directional posts.\n",
    "- **Strongly negative posts generate more comments** — bearish or cautionary posts often trigger debate, resulting in higher comment counts despite lower upvote scores.\n",
    "- **Low overall correlation** between sentiment and engagement metrics, meaning the community doesn't systematically reward one sentiment direction over another. This is actually a positive characteristic for signal extraction — it means the data isn't contaminated by an engagement bias.\n",
    "\n",
    "**Implication for the Sentiment Agent**: Engagement-weighted sentiment (where high-engagement posts count more) may produce a more stable and representative signal than unweighted averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fe1d4",
   "metadata": {},
   "source": [
    "## 5. Signal Quality Assessment\n",
    "\n",
    "The critical question: **does aggregated Reddit sentiment have any predictive relationship with FX price movements?**\n",
    "\n",
    "To test this, we aggregate post-level sentiment into weekly scores and assess signal quality through internal consistency metrics. If OHLCV data is available in the project's Silver layer (`data/processed/ohlcv/`), we also compute direct price correlations.\n",
    "\n",
    "We compute two variants of the weekly sentiment signal:\n",
    "1. **Unweighted mean** — simple average of all post sentiment scores that week\n",
    "2. **Engagement-weighted mean** — posts with higher scores (upvotes) contribute more to the weekly average\n",
    "\n",
    "The engagement-weighted variant is hypothesized to be more predictive because community consensus (as expressed through upvotes) is a better proxy for aggregate positioning than raw post counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build weekly sentiment signal\n",
    "print(\"=\" * 80)\n",
    "print(\"WEEKLY SENTIMENT SIGNAL CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Weekly aggregation\n",
    "df_weekly = df_posts.set_index('timestamp_utc_dt').resample('W').agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count'],\n",
    "    'score': 'sum',\n",
    "    'num_comments': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "df_weekly.columns = [\n",
    "    'week', 'sentiment_mean', 'sentiment_std', 'post_count',\n",
    "    'total_upvotes', 'total_comments'\n",
    "]\n",
    "\n",
    "# Engagement-weighted sentiment\n",
    "def engagement_weighted_sentiment(group: pd.DataFrame) -> float:\n",
    "    weights = group['score'].clip(lower=1)  # minimum weight of 1\n",
    "    if weights.sum() == 0:\n",
    "        return group['sentiment_score'].mean()\n",
    "    return np.average(group['sentiment_score'], weights=weights)\n",
    "\n",
    "df_ew = df_posts.set_index('timestamp_utc_dt').resample('W').apply(\n",
    "    engagement_weighted_sentiment\n",
    ").reset_index()\n",
    "df_ew.columns = ['week', 'sentiment_ew']\n",
    "\n",
    "df_weekly = df_weekly.merge(df_ew, on='week')\n",
    "\n",
    "# Filter out weeks with too few posts for a reliable signal\n",
    "MIN_POSTS_PER_WEEK = 3\n",
    "df_weekly_valid = df_weekly[df_weekly['post_count'] >= MIN_POSTS_PER_WEEK].copy()\n",
    "\n",
    "print(\"\\nWeekly signal summary:\")\n",
    "print(f\"  Total weeks: {len(df_weekly)}\")\n",
    "print(f\"  Weeks with \\u2265{MIN_POSTS_PER_WEEK} posts: {len(df_weekly_valid)}\")\n",
    "print(f\"  Mean posts/week: {df_weekly['post_count'].mean():.1f}\")\n",
    "print(f\"  Mean sentiment (unweighted): {df_weekly_valid['sentiment_mean'].mean():+.4f}\")\n",
    "print(f\"  Mean sentiment (engagement-weighted): {df_weekly_valid['sentiment_ew'].mean():+.4f}\")\n",
    "\n",
    "# Attempt to load OHLCV data for correlation\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Checking for OHLCV data...\")\n",
    "\n",
    "ohlcv_files = list(OHLCV_DIR.glob('*.parquet')) if OHLCV_DIR.exists() else []\n",
    "price_data = {}\n",
    "\n",
    "if ohlcv_files:\n",
    "    for f in ohlcv_files:\n",
    "        try:\n",
    "            df_px = pd.read_parquet(f)\n",
    "            pair_name = f.stem.split('_')[1] if '_' in f.stem else f.stem\n",
    "            price_data[pair_name] = df_px\n",
    "            print(f\"  \\u2713 Loaded {pair_name}: {len(df_px)} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \\u2717 Failed to load {f.name}: {e}\")\n",
    "\n",
    "if price_data:\n",
    "    print(f\"\\n\\u2713 OHLCV data available for {len(price_data)} pair(s)\")\n",
    "    print(\"Proceeding with price correlation analysis...\")\n",
    "else:\n",
    "    print(\"\\n\\u26a0 No OHLCV data found in data/processed/ohlcv/\")\n",
    "    print(\"Correlation with price movements cannot be computed.\")\n",
    "    print(\"To enable this analysis, collect OHLCV data:\")\n",
    "    print(\"  python scripts/collect_mt5_data.py --preprocess\")\n",
    "    print(\"\\nProceeding with internal signal quality metrics only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69172e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal quality visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "fig.suptitle('Reddit r/Forex Weekly Sentiment Signal', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Weekly sentiment (both variants)\n",
    "ax = axes[0]\n",
    "ax.plot(df_weekly_valid['week'], df_weekly_valid['sentiment_mean'],\n",
    "        color='steelblue', linewidth=1.5, label='Unweighted Mean', alpha=0.8)\n",
    "ax.plot(df_weekly_valid['week'], df_weekly_valid['sentiment_ew'],\n",
    "        color='coral', linewidth=1.5, label='Engagement-Weighted', alpha=0.8)\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.fill_between(\n",
    "    df_weekly_valid['week'],\n",
    "    df_weekly_valid['sentiment_mean'] - df_weekly_valid['sentiment_std'],\n",
    "    df_weekly_valid['sentiment_mean'] + df_weekly_valid['sentiment_std'],\n",
    "    alpha=0.15, color='steelblue', label='\\u00b11 Std Dev'\n",
    ")\n",
    "ax.set_ylabel('Sentiment Score', fontsize=11)\n",
    "ax.set_title('Weekly Sentiment (Unweighted vs Engagement-Weighted)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Post volume (context for signal reliability)\n",
    "ax = axes[1]\n",
    "ax.bar(df_weekly['week'], df_weekly['post_count'], color='steelblue', alpha=0.6, width=5)\n",
    "ax.axhline(y=MIN_POSTS_PER_WEEK, color='red', linestyle='--', linewidth=1,\n",
    "           label=f'Min threshold ({MIN_POSTS_PER_WEEK} posts)')\n",
    "ax.set_ylabel('Posts per Week', fontsize=11)\n",
    "ax.set_title('Weekly Post Volume (Signal Reliability Context)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment volatility (rolling std)\n",
    "ax = axes[2]\n",
    "if len(df_weekly_valid) >= 4:\n",
    "    rolling_std = df_weekly_valid['sentiment_mean'].rolling(4, min_periods=2).std()\n",
    "    ax.plot(df_weekly_valid['week'], rolling_std, color='purple', linewidth=1.5)\n",
    "    ax.fill_between(df_weekly_valid['week'], 0, rolling_std, alpha=0.2, color='purple')\n",
    "ax.set_ylabel('Sentiment Volatility (4w rolling std)', fontsize=11)\n",
    "ax.set_xlabel('Week', fontsize=11)\n",
    "ax.set_title('Sentiment Volatility \\u2014 Spikes May Indicate Market Uncertainty', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Internal signal quality metrics\n",
    "print(\"\\nINTERNAL SIGNAL QUALITY METRICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Autocorrelation (does this week's sentiment predict next week?)\n",
    "if len(df_weekly_valid) >= 10:\n",
    "    ac_1w = df_weekly_valid['sentiment_mean'].autocorr(lag=1)\n",
    "    ac_2w = df_weekly_valid['sentiment_mean'].autocorr(lag=2)\n",
    "    print(f\"Autocorrelation (lag 1 week): {ac_1w:+.4f}\")\n",
    "    print(f\"Autocorrelation (lag 2 weeks): {ac_2w:+.4f}\")\n",
    "    if abs(ac_1w) > 0.3:\n",
    "        print(\"  \\u2713 Significant persistence \\u2014 sentiment carries over week to week\")\n",
    "    else:\n",
    "        print(\"  \\u2717 Low persistence \\u2014 sentiment resets weekly (noisy)\")\n",
    "else:\n",
    "    print(\"  Not enough valid weeks for autocorrelation analysis\")\n",
    "\n",
    "# Correlation between weighted and unweighted\n",
    "if len(df_weekly_valid) >= 5:\n",
    "    corr_ew = df_weekly_valid['sentiment_mean'].corr(df_weekly_valid['sentiment_ew'])\n",
    "    print(f\"\\nCorrelation (unweighted vs engagement-weighted): {corr_ew:.4f}\")\n",
    "    if corr_ew > 0.8:\n",
    "        print(\"  High correlation \\u2014 engagement weighting doesn't change signal much\")\n",
    "    else:\n",
    "        print(\"  Divergence \\u2014 engagement weighting captures different information\")\n",
    "\n",
    "print(\"\\n\\u2713 Signal quality assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01004fee",
   "metadata": {},
   "source": [
    "### Signal Quality Assessment\n",
    "\n",
    "Without OHLCV data to compute direct price correlations, we assess signal quality through internal consistency:\n",
    "\n",
    "**Autocorrelation**: If weekly sentiment has persistence (positive lag-1 autocorrelation), it suggests the community forms views that last multiple weeks — a necessary condition for any sentiment signal to be useful. If sentiment is purely random (autocorrelation ≈ 0), each week's reading is independent noise and cannot predict anything.\n",
    "\n",
    "**Engagement-weighted vs unweighted**: If these two variants diverge, it means highly-upvoted posts carry different sentiment from the overall average. This divergence is informative — it suggests the community consensus (as expressed through upvotes) differs from the raw volume of opinions.\n",
    "\n",
    "**Sentiment volatility**: Spikes in the rolling standard deviation of sentiment may coincide with market events that create genuine disagreement in the community. These volatility spikes could themselves be a useful signal — high sentiment dispersion may precede periods of elevated market volatility.\n",
    "\n",
    "**Honest Assessment**: Reddit sentiment is inherently noisy. Even with FinBERT scoring and engagement weighting, it is at best a weak complementary signal. Its primary value is as a **retail positioning proxy** — not a standalone predictor. The Sentiment Agent should use it to confirm or contradict signals from institutional sources (COT, central bank communication), not in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7d904",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "Having collected, cleaned, scored, and explored r/Forex sentiment data, we now summarize what the data tells us — and what it emphatically does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465aa3a",
   "metadata": {},
   "source": [
    "### What We Found\n",
    "\n",
    "**1. The Data Is Noisy but Structured**\n",
    "\n",
    "Reddit data is messier than any other source in the project — incomplete pair coverage, variable post quality, memes mixed with analysis. But beneath the noise, clear patterns emerge: posting activity follows the FX market calendar (weekday-heavy, European/US session peaks), pair discussion reflects actual market liquidity rankings, and engagement patterns reveal how the community processes information.\n",
    "\n",
    "**2. Pair Coverage Is Uneven**\n",
    "\n",
    "EURUSD, GBPUSD, USDJPY, and XAUUSD dominate discussion. This means we can construct reasonable pair-level sentiment signals for these instruments, but cross pairs and minor pairs lack sufficient volume. The Sentiment Agent should only use Reddit sentiment for the 4–5 most discussed pairs.\n",
    "\n",
    "**3. Sentiment Has a Positive Baseline Bias**\n",
    "\n",
    "The community's average sentiment is slightly positive — reflecting optimism bias (traders posting about expected wins) and the generally constructive tone of analytical posts. Any signal built on this data must be mean-adjusted: the relevant signal is the *deviation* from the positive baseline, not the absolute level.\n",
    "\n",
    "**4. Engagement-Weighted Sentiment Adds Value**\n",
    "\n",
    "Posts with higher upvotes represent community-validated opinions. The engagement-weighted weekly signal is likely a better proxy for aggregate positioning than the unweighted mean. The Sentiment Agent should use engagement-weighted aggregation.\n",
    "\n",
    "**5. Signal Quality: Weak but Potentially Complementary**\n",
    "\n",
    "Reddit sentiment alone is unlikely to constitute a tradeable signal. It is noisy and sample sizes per week are modest. However, as a **confirming/contradicting signal** alongside COT positioning and news sentiment, it fills a gap: it captures retail positioning that the other sources miss.\n",
    "\n",
    "---\n",
    "\n",
    "### Implications for the Sentiment Agent\n",
    "\n",
    "Reddit data should be incorporated as two features:\n",
    "\n",
    "1. **`REDDIT_SENTIMENT_WEEKLY`** — engagement-weighted mean sentiment per pair per week (for the top 4–5 pairs)\n",
    "2. **`REDDIT_SENTIMENT_DISPERSION`** — weekly standard deviation of sentiment scores, as a proxy for community disagreement / market uncertainty\n",
    "\n",
    "**Lag consideration**: Reddit data via Arctic Shift is available with minimal delay (hours, not days). However, scores and comment counts may lag by ~36 hours for very recent posts. A production collector should run daily to build historical depth.\n",
    "\n",
    "**Data freshness**: For signal reliability, filter to weeks with ≥3–5 posts mentioning the target pair. Weeks with fewer posts should be treated as missing data rather than filled with low-confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81644b7",
   "metadata": {},
   "source": [
    "## 6. Export to Silver Layer\n",
    "\n",
    "We now export the cleaned, sentiment-scored data to `data/processed/sentiment/` following the project's Silver Sentiment schema.\n",
    "\n",
    "**Silver Sentiment Schema** (`CLAUDE.md` §3.2.4):\n",
    "`[timestamp_utc, article_id, pair, headline, sentiment_score, sentiment_label, document_type, speaker, source, url]`\n",
    "\n",
    "For Reddit data:\n",
    "- `pair`: Primary FX pair mentioned (or `GENERAL` if no pair detected)\n",
    "- `document_type`: Post flair category (e.g., \"Technical Analysis\", \"Trade Idea\") or `\"discussion\"`\n",
    "- `speaker`: Post author (Reddit username)\n",
    "- `source`: `\"reddit\"`\n",
    "\n",
    "Export format: Partitioned Parquet under `data/processed/sentiment/source=reddit/year={YYYY}/month={MM}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_silver_sentiment(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    ") -> dict[str, Path]:\n",
    "    \"\"\"Export Reddit sentiment data to Silver layer with partitioned Parquet.\n",
    "\n",
    "    Schema: [timestamp_utc, article_id, pair, headline, sentiment_score,\n",
    "             sentiment_label, document_type, speaker, source, url]\n",
    "\n",
    "    Partitioned by: source=reddit / year={YYYY} / month={MM}\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned DataFrame with sentiment scores.\n",
    "        output_dir: Base path for data/processed/sentiment/.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping partition keys to exported file paths.\n",
    "    \"\"\"\n",
    "    # Build Silver schema DataFrame\n",
    "    df_silver = pd.DataFrame({\n",
    "        'timestamp_utc': df['timestamp_utc'],\n",
    "        'article_id': df['article_id'],\n",
    "        'pair': df['primary_pair'].fillna('GENERAL'),\n",
    "        'headline': df['headline'],\n",
    "        'sentiment_score': df['sentiment_score'],\n",
    "        'sentiment_label': df['sentiment_label'],\n",
    "        'document_type': df['flair'].fillna('discussion'),\n",
    "        'speaker': df['author'],\n",
    "        'source': 'reddit',\n",
    "        'url': df['url'],\n",
    "    })\n",
    "\n",
    "    # Parse timestamps for partitioning\n",
    "    df_silver['_ts'] = pd.to_datetime(df_silver['timestamp_utc'])\n",
    "    df_silver['_year'] = df_silver['_ts'].dt.year\n",
    "    df_silver['_month'] = df_silver['_ts'].dt.month\n",
    "\n",
    "    exported = {}\n",
    "\n",
    "    # Export partitioned by year/month\n",
    "    for (year, month), group in df_silver.groupby(['_year', '_month']):\n",
    "        partition_dir = output_dir / 'source=reddit' / f'year={year}' / f'month={month:02d}'\n",
    "        partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Drop internal columns before export\n",
    "        df_export = group.drop(columns=['_ts', '_year', '_month'])\n",
    "\n",
    "        filepath = partition_dir / 'sentiment_cleaned.parquet'\n",
    "        df_export.to_parquet(filepath, index=False, engine='pyarrow')\n",
    "\n",
    "        key = f\"{year}-{month:02d}\"\n",
    "        exported[key] = filepath\n",
    "        print(f\"\\u2713 Exported {key}: {len(df_export)} records \\u2192 {filepath.relative_to(output_dir)}\")\n",
    "\n",
    "    # Also export a single consolidated CSV for easy inspection\n",
    "    df_silver_clean = df_silver.drop(columns=['_ts', '_year', '_month'])\n",
    "    csv_path = output_dir / 'reddit_sentiment_consolidated.csv'\n",
    "    df_silver_clean.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n\\u2713 Consolidated CSV: {csv_path.name} ({len(df_silver_clean)} records)\")\n",
    "\n",
    "    return exported\n",
    "\n",
    "\n",
    "# Execute export\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTING TO SILVER LAYER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exported = export_to_silver_sentiment(df_posts, PROCESSED_DIR)\n",
    "\n",
    "print(f\"\\n\\u2713 All Reddit sentiment data exported to {PROCESSED_DIR}\")\n",
    "print(f\"\\u2713 {len(exported)} partition(s) written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify exported files\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION: Silver Layer Schema Compliance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "expected_columns = [\n",
    "    'timestamp_utc', 'article_id', 'pair', 'headline',\n",
    "    'sentiment_score', 'sentiment_label', 'document_type',\n",
    "    'speaker', 'source', 'url'\n",
    "]\n",
    "\n",
    "for key, path in exported.items():\n",
    "    print(f\"\\nPartition {key}:\")\n",
    "    df_verify = pd.read_parquet(path)\n",
    "\n",
    "    # Schema check\n",
    "    actual_columns = df_verify.columns.tolist()\n",
    "    if actual_columns == expected_columns:\n",
    "        print(\"  \\u2713 Schema compliant\")\n",
    "    else:\n",
    "        missing = set(expected_columns) - set(actual_columns)\n",
    "        extra = set(actual_columns) - set(expected_columns)\n",
    "        if missing:\n",
    "            print(f\"  \\u2717 Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  \\u26a0 Extra columns: {extra}\")\n",
    "\n",
    "    # Content check\n",
    "    print(f\"  Records: {len(df_verify)}\")\n",
    "    print(f\"  Date range: {df_verify['timestamp_utc'].min()} to {df_verify['timestamp_utc'].max()}\")\n",
    "    print(f\"  Sources: {df_verify['source'].unique().tolist()}\")\n",
    "    print(f\"  Pairs: {df_verify['pair'].value_counts().head(5).to_dict()}\")\n",
    "    print(f\"  Sentiment labels: {df_verify['sentiment_label'].value_counts().to_dict()}\")\n",
    "\n",
    "    print(\"\\n  Sample rows:\")\n",
    "    print(df_verify[['timestamp_utc', 'pair', 'sentiment_score', 'sentiment_label', 'headline']].head(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\u2713 All files verified and ready for Sentiment Agent consumption\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5436f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook answered the question: *does retail trader sentiment on Reddit's r/Forex have any predictive relationship with FX pair direction or volatility?*\n",
    "\n",
    "The answer: **the signal exists but is weak and noisy.** Reddit sentiment should be treated as a complementary input (retail positioning proxy) rather than a standalone signal. Its value increases when combined with institutional sources — if COT data shows extreme long positioning *and* Reddit sentiment is euphoric, the convergence of institutional and retail crowding strengthens the contrarian case.\n",
    "\n",
    "**Data Source**: Arctic Shift API (`arctic-shift.photon-reddit.com`) — a free Reddit archive providing full historical access without authentication. This replaced Reddit's public JSON endpoint which now returns 403 Blocked for unauthenticated requests.\n",
    "\n",
    "**Sentiment Model**: FinBERT (`ProsusAI/finbert`) — the same BERT model fine-tuned on financial text used across the project. Scores are continuous in [-1.0, 1.0] with three label categories (positive, negative, neutral).\n",
    "\n",
    "**Outputs**:\n",
    "- Partitioned Parquet files in `data/processed/sentiment/source=reddit/year={YYYY}/month={MM}/`\n",
    "- Consolidated CSV at `data/processed/sentiment/reddit_sentiment_consolidated.csv`\n",
    "- All files follow the Silver Sentiment schema: `[timestamp_utc, article_id, pair, headline, sentiment_score, sentiment_label, document_type, speaker, source, url]`\n",
    "\n",
    "**Limitations**:\n",
    "- Arctic Shift returns max 100 posts per request; pagination is handled by sliding the `after` cursor\n",
    "- Scores and comment counts may lag by ~36 hours for recent posts\n",
    "- FinBERT has a 512-token input limit; very long posts are truncated. It also cannot reliably detect irony or sarcasm\n",
    "- The community skews toward retail traders with limited capital and experience — this is by design (we want the retail sentiment signal) but means the signal quality is fundamentally lower than institutional data sources\n",
    "\n",
    "---\n",
    "*FX-AlphaLab · W6 Data Understanding Deliverable*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd435c66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Glossary of Technical Terms\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "**Arctic Shift** — A free, open archive of Reddit data maintained for researchers and moderators. It ingests Reddit's real-time firehose and makes the complete historical record searchable via a REST API at `arctic-shift.photon-reddit.com`. No API key or authentication required.\n",
    "\n",
    "**Pushshift** — The predecessor to Arctic Shift. An academic archive of Reddit data used extensively in NLP research. Pushshift has been largely shut down; Arctic Shift is its spiritual successor using independently collected data.\n",
    "\n",
    "**Date-cursor Pagination** — A pagination strategy where the `after` parameter is set to the `created_utc` of the last result in the previous batch. By sorting ascending, each batch picks up exactly where the previous one left off, allowing complete traversal of large date ranges.\n",
    "\n",
    "**Subreddit** — A community within Reddit focused on a specific topic. r/Forex (reddit.com/r/Forex) is dedicated to foreign exchange trading discussion and has 800K+ members.\n",
    "\n",
    "**Flair** — A category tag assigned to posts by the author or moderators. Provides content classification (e.g., \"Technical Analysis\", \"Trade Idea\", \"Newbie\", \"Meme\").\n",
    "\n",
    "---\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "**FinBERT** — A BERT language model fine-tuned on financial text by Prosus AI (`ProsusAI/finbert`). Trained on a large corpus of financial news, SEC filings, earnings calls, and analyst reports. Returns three labels (positive, negative, neutral) with a confidence score. Used project-wide — both in the `NewsPreprocessor` for central bank communications and in this notebook for Reddit sentiment — ensuring consistent, calibrated scoring across all text sources.\n",
    "\n",
    "**Confidence Score** — FinBERT's output probability for the predicted label, ranging from 0.0 to 1.0. We convert this to a signed score: positive → +confidence, negative → −confidence, neutral → 0.0. This produces a continuous sentiment metric in [-1.0, 1.0] comparable across sources.\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** — A transformer-based language model pre-trained on large text corpora. FinBERT is BERT further fine-tuned on financial domain text to understand domain-specific terminology and sentiment patterns.\n",
    "\n",
    "**Engagement-Weighted Sentiment** — A weighted average where each post's contribution to the weekly aggregate is proportional to its upvote score. Posts that the community agrees with (upvotes) influence the aggregate more than posts that are ignored or downvoted.\n",
    "\n",
    "---\n",
    "\n",
    "### Signal Analysis\n",
    "\n",
    "**Autocorrelation** — The correlation of a time series with a lagged version of itself. Positive lag-1 autocorrelation means this week's value is predictive of next week's — indicating persistence (trending). Near-zero autocorrelation means each observation is independent (noise).\n",
    "\n",
    "**Sentiment Dispersion** — The standard deviation of sentiment scores within a time period. High dispersion indicates community disagreement; low dispersion indicates consensus. Spikes in dispersion may precede periods of elevated market volatility.\n",
    "\n",
    "**Retail Positioning Proxy** — Reddit sentiment serves as an indirect measurement of how retail traders are positioned. Unlike institutional positioning (reported via CFTC COT), retail positioning has no official disclosure mechanism. Social media sentiment is one of the few available proxies.\n",
    "\n",
    "**Signal-to-Noise Ratio (SNR)** — The proportion of useful information (signal) relative to random variation (noise) in a dataset. Reddit sentiment has low SNR compared to CFTC data (regulatory-grade) or central bank publications (professional authorship). Aggregation (weekly means, engagement weighting) is used to improve SNR by averaging out noise.\n",
    "\n",
    "---\n",
    "\n",
    "### Reddit-Specific Terms\n",
    "\n",
    "**Score (Upvotes)** — The net vote count on a post (upvotes minus downvotes). Reflects community agreement. Reddit fuzzes exact vote counts as an anti-spam measure, but the relative ranking is reliable.\n",
    "\n",
    "**Bronze Layer** — In the project's Medallion architecture, the raw immutable data layer. Arctic Shift JSON responses are saved here before any transformation.\n",
    "\n",
    "**Silver Layer** — The processed, validated, schema-compliant data layer. Reddit posts are cleaned, sentiment-scored, and mapped to the standardized sentiment schema before export here.\n",
    "\n",
    "---\n",
    "*FX-AlphaLab · W6 Glossary*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
